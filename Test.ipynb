{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb053727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement utils.img_pool (from versions: none)\n",
      "ERROR: No matching distribution found for utils.img_pool\n"
     ]
    }
   ],
   "source": [
    "pip install utils.img_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b002e12",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.img_pool'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimg_pool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImagePool\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils.img_pool'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils import data\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset\n",
    "from utils.img_pool import ImagePool\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "\n",
    "#%config.InlineBackend.figure_format = 'retina'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"7\"\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71b4469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "epochs = 30\n",
    "num_pairs = 200\n",
    "num_residual_blocks = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aec26efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used since implented as a module below.\n",
    "def residual_block(inp, num_features):\n",
    "    \"\"\"\n",
    "    Transformation step of the generator.\n",
    "    \n",
    "        inp: Input tensor to the residual block.\n",
    "    \"\"\"\n",
    "    in_ch = inp.shape[1]  # Num of channels of the input tensor.\n",
    "\n",
    "    conv1 = nn.Conv2d(in_channels=in_ch, out_channels=num_features, kernel_size=(3,3), stride=1, padding=1)\n",
    "    conv2 = nn.Conv2d(in_channels=num_features, out_channels=num_features, kernel_size=(3,3), stride=1, padding=1)\n",
    "\n",
    "    return conv2(conv1(inp) + inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1b6c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channel, out_channel, activation='relu', *args, **kwargs):\n",
    "\n",
    "    activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU(negative_slope=0.2, inplace=True)],\n",
    "                ['relu', nn.ReLU()]\n",
    "    ])\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channel, out_channel, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_channel),\n",
    "        activations[activation]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bb8d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                     stride=stride, padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "531cd830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_pairs(data_dir, num_pairs=10, train_val_ratio=0.9):\n",
    "\n",
    "    # Determine the amount of pictures that will take place in train and validation sets\n",
    "    num_pairs_train = int(num_pairs * train_val_ratio)\n",
    "\n",
    "    # Read csv that comes with the celebA dataset.\n",
    "    d = pd.read_csv('{}/list_attr_celeba.csv'.format(data_dir))\n",
    "\n",
    "    # Get image_id and gender label (both type str) into numpy arrays for later masking.\n",
    "    males = np.array(d.nlargest(num_pairs, 'Male').get('image_id').values)\n",
    "    females = np.array(d.nsmallest(num_pairs, 'Male').get('image_id').values)\n",
    "\n",
    "    # Generate random index array.\n",
    "    idx = np.arange(num_pairs)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    # Create the random mask for both datasets.\n",
    "    train_mask = idx[:num_pairs_train]\n",
    "    val_mask = idx[num_pairs_train:]\n",
    "\n",
    "    # Get the masked male image_ids for train and val datasets.\n",
    "    train_male_ids = males[train_mask]\n",
    "    val_male_ids = males[val_mask]\n",
    "\n",
    "    # Get the masked female image_ids for train and val datasets.\n",
    "    train_female_ids = females[train_mask]\n",
    "    val_female_ids = females[val_mask]\n",
    "\n",
    "    # Return image_id lists as dictionaries.\n",
    "    tr_pairs = {'males': list(train_male_ids), 'females': list(train_female_ids)}\n",
    "    val_pairs = {'males': list(val_male_ids), 'females': list(val_female_ids)}\n",
    "\n",
    "    return tr_pairs, val_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8e8866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_images(im_list, save_dir, epoch_num, save_mode_on=True):\n",
    "    \"\"\"\n",
    "        Pytorch conv2d uses input & output dimensions as: (N, C, H, W).\n",
    "        To be able to plot the generated images, torch tensors must be converted back to (W,H,C)\n",
    "        and transferred back into the local memory by .cpu() function\n",
    "    \"\"\"\n",
    "\n",
    "    # A list that holds the necessary plot titles\n",
    "    titles = ['Real-A', 'Fake-B (A->B)', 'Recon-A (A->B->A)', 'Identity-A (A->A)',\n",
    "              'Real-B', 'Fake-A (B->A)', 'Recon-B (B->A->B)', 'Identity-B (B->B)']\n",
    "\n",
    "    # Plot output images in one master figure as subplots\n",
    "    im_idx = 0\n",
    "    fig, axarr = plt.subplots(2,4, figsize=(12, 6))\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            # Adjust network output image to proper dimensions.\n",
    "            im = im_list[im_idx].squeeze().T\n",
    "            # Scale from [-1..1] to [0..1] for plotting.\n",
    "            im = (im + 1) / 2.0\n",
    "            # Remove the numbers from the axes.\n",
    "            axarr[i, j].axis('off')\n",
    "            axarr[i, j].imshow(im.detach().cpu(), vmin=0, vmax=1)\n",
    "            axarr[i, j].set_title(titles[im_idx], fontweight=\"bold\")\n",
    "\n",
    "            im_idx = im_idx + 1\n",
    "\n",
    "    #plt.subplots_adjust(hspace=0.0002)  # Set spacing between subplots.\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Either save figures or just plot them\n",
    "    if save_mode_on:\n",
    "        plt.savefig(os.path.join(save_dir, 'epoch-{}.jpg'.format(epoch_num)))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd98ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_model_id(model_save_dir):\n",
    "    \"\"\"This method assigns a proper name to the model that will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    return 'ep_' + str(epochs) + '-pairs_' + str(num_pairs) + '-resblocks_' + str(num_residual_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3823ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-Do: Revise and decide relu and/or bnorm are needed or not.\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c50babe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "\n",
    "    model = nn.Sequential(OrderedDict([]))\n",
    "\n",
    "    # Encoding\n",
    "    encoder = nn.Sequential(OrderedDict([\n",
    "        ('conv1', nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(7,7), stride=1, bias=False, padding=3)),\n",
    "        ('bnorm1', nn.BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "        ('relu1', nn.ReLU()),\n",
    "        ('conv2', nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), stride=2, bias=False, padding=1)),\n",
    "        ('bnorm2', nn.BatchNorm2d(num_features=128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "        ('relu2', nn.ReLU()),\n",
    "        ('conv3', nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3), stride=2, bias=False, padding=1)),\n",
    "        ('bnorm3', nn.BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "        ('relu3', nn.ReLU())\n",
    "        ]))\n",
    "    model.add_module(name='encoder', module=encoder)\n",
    "\n",
    "    # Transformation\n",
    "    for i in range(num_residual_blocks):\n",
    "        model.add_module(name='res{}'.format(i+1), module=ResidualBlock(256,256))\n",
    "\n",
    "    # Decoding\n",
    "    decoder = nn.Sequential(OrderedDict([\n",
    "        ('deconv1', nn.ConvTranspose2d(256,64, kernel_size=(3,3), stride=2, padding=1, output_padding=1)),\n",
    "        ('bnorm4', nn.BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "        ('relu4', nn.ReLU()),\n",
    "        ('deconv2', nn.ConvTranspose2d(64,32, kernel_size=(3,3), stride=2, padding=1, output_padding=1)),\n",
    "        ('bnorm5', nn.BatchNorm2d(num_features=32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "        ('relu5', nn.ReLU()),\n",
    "        ('reflectpad', nn.ReflectionPad2d(3)),\n",
    "        ('conv4', nn.Conv2d(in_channels=32, out_channels=3, kernel_size=(7,7), stride=1, bias=True)),\n",
    "        ('tanh', nn.Tanh())\n",
    "    ]))\n",
    "    model.add_module(name='decoder', module=decoder)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aeac9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the original code lrelus are: LeakyReLU(0.2, True)\n",
    "def create_discriminator():\n",
    "    # Last conv2d outputs a patch 30x30 as prediction matrix.\n",
    "\n",
    "    discriminator = nn.Sequential(OrderedDict([\n",
    "        ('conv', nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(4,4), stride=2, padding=1)),\n",
    "        ('lrelu1', nn.LeakyReLU(0.2, True)),\n",
    "        ('convblock1', conv_block(in_channel=64, out_channel=128, activation='lrelu', kernel_size=(4,4), stride=2, padding=1, bias=False)),\n",
    "        ('convblock2', conv_block(in_channel=128, out_channel=256, activation='lrelu', kernel_size=(4,4), stride=2, padding=1, bias=False)),\n",
    "        ('convblock3', conv_block(in_channel=256, out_channel=512, activation='lrelu', kernel_size=(4,4), stride=1, padding=1, bias=False)),\n",
    "        ('patch', nn.Conv2d(in_channels=512, out_channels=1, kernel_size=(4,4), stride=1, padding=1))\n",
    "    ]))  # Out shape: [1, 1, 30, 30]\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86fb9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_loss(pred, is_real):\n",
    "\n",
    "    # Typical GAN loss to set objectives for generator and discriminator\n",
    "    if is_real:\n",
    "        # Ex: torch.ones([2, 4], dtype=torch.float64, device=cuda0)\n",
    "        return F.mse_loss(pred, torch.ones(pred.shape).to(device))\n",
    "    else:\n",
    "        return F.mse_loss(pred, torch.zeros(pred.shape).to(device))\n",
    "\n",
    "def cycle_loss(reconstructed_images, real_images):\n",
    "\n",
    "    # Cycle loss to make sure reconstructed image looks real\n",
    "    return F.l1_loss(reconstructed_images, real_images)\n",
    "\n",
    "def identity_loss(identity_images, real_images):\n",
    "\n",
    "    # Identity loss to make sure generator won't do unnecessary change\n",
    "    # Ideally, feeding a real image to generator should generate itself\n",
    "    return F.l1_loss(identity_images, real_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "761a4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cycleGAN(nn.Module):\n",
    "\n",
    "    def __init__(self, learning_rate=2e-4):\n",
    "\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Loss function coeffs\n",
    "        self.LAMBDA_CYCLE = 10.0\n",
    "        self.LAMBDA_ID = 0.5\n",
    "\n",
    "        # Image pool parameter\n",
    "        pool_size = 50\n",
    "\n",
    "        # Discriminate validation and train behaviour\n",
    "        self.is_training = True\n",
    "        self.save_losses = False\n",
    "\n",
    "        # Initialize the image pools for both domains.\n",
    "        self.fake_A_pool = ImagePool(pool_size)\n",
    "        self.fake_B_pool = ImagePool(pool_size)\n",
    "\n",
    "        # Create dictionaries to save the entire loss progress\n",
    "        self.tr_gen_loss_dict = {\n",
    "            'loss_gen_a2b': [],\n",
    "            'loss_gen_b2a': [],\n",
    "            'loss_id_a2b': [],\n",
    "            'loss_id_b2a': [],\n",
    "            'loss_cycle_a2b2a': [],\n",
    "            'loss_cycle_b2a2b': [],\n",
    "            'loss_gen_total': []\n",
    "        }\n",
    "        self.tr_dis_loss_dict = {\n",
    "            'loss_dis_b': [],\n",
    "            'loss_dis_a': [],\n",
    "            'loss_dis_total': []\n",
    "        }\n",
    "        self.val_gen_loss_dict = {\n",
    "            'loss_gen_a2b': [],\n",
    "            'loss_gen_b2a': [],\n",
    "            'loss_id_a2b': [],\n",
    "            'loss_id_b2a': [],\n",
    "            'loss_cycle_a2b2a': [],\n",
    "            'loss_cycle_b2a2b': [],\n",
    "            'loss_gen_total': []\n",
    "        }\n",
    "        self.val_dis_loss_dict = {\n",
    "            'loss_dis_b': [],\n",
    "            'loss_dis_a': [],\n",
    "            'loss_dis_total': []\n",
    "        }\n",
    "#         self.gen_loss_dict = {}\n",
    "#         self.dis_loss_dict = {}\n",
    "        self.im_list = []\n",
    "\n",
    "        self.generator_a2b = create_generator()\n",
    "        self.generator_b2a = create_generator()\n",
    "\n",
    "        self.discriminator_a = create_discriminator()\n",
    "        self.discriminator_b = create_discriminator()\n",
    "\n",
    "        # To-Do: Set optimizers' lr and betas parameters.\n",
    "        self.optimizer_G = torch.optim.Adam(itertools.chain(self.generator_a2b.parameters(), self.generator_b2a.parameters()), lr=self.learning_rate)\n",
    "        self.optimizer_D = torch.optim.Adam(itertools.chain(self.discriminator_a.parameters(), self.discriminator_b.parameters()), lr=self.learning_rate)\n",
    "\n",
    "    def forward(self, real_a, real_b):\n",
    "\n",
    "        # Cycle A -> B -> A\n",
    "        fake_a2b = self.generator_a2b(real_a)\n",
    "        recon_b2a = self.generator_b2a(fake_a2b)\n",
    "\n",
    "        # Cycle B -> A -> B\n",
    "        fake_b2a = self.generator_b2a(real_b)\n",
    "        recon_a2b = self.generator_a2b(fake_b2a)\n",
    "\n",
    "        # Use real B to generate B should be identical\n",
    "        identity_a2b = self.generator_a2b(real_b)\n",
    "        identity_b2a = self.generator_b2a(real_a)\n",
    "\n",
    "        # Save images in an ordered list to be printed at the end of each epoch.\n",
    "        self.im_list = [real_a, fake_a2b, recon_b2a, identity_b2a,\n",
    "                        real_b, fake_b2a, recon_a2b, identity_a2b]\n",
    "\n",
    "        return fake_a2b, recon_b2a, fake_b2a, recon_a2b, identity_a2b, identity_b2a\n",
    "\n",
    "    def backward_G(self, real_a, real_b, fake_a2b, recon_b2a, fake_b2a, recon_a2b, identity_a2b, identity_b2a):\n",
    "        # To-Do: Move external loss funcs into the class.\n",
    "\n",
    "        if self.is_training:\n",
    "            # Ds require no gradients when optimizing Gs\n",
    "            self.set_requires_grad([self.discriminator_a, self.discriminator_b], False)\n",
    "            # Set G_A and G_B's gradients to zero\n",
    "            self.optimizer_G.zero_grad()\n",
    "\n",
    "        loss_identity_a2b = identity_loss(identity_a2b, real_b)\n",
    "        loss_identity_b2a = identity_loss(identity_b2a, real_a)\n",
    "\n",
    "        # Generator A2B tries to trick Discriminator B that the generated image is B\n",
    "        loss_gan_gen_a2b = gan_loss(self.discriminator_b(fake_a2b), True)\n",
    "        # Generator B2A tries to trick Discriminator A that the generated image is A\n",
    "        loss_gan_gen_b2a = gan_loss(self.discriminator_a(fake_b2a), True)\n",
    "        loss_cycle_a2b2a = cycle_loss(recon_b2a, real_a)\n",
    "        loss_cycle_b2a2b = cycle_loss(recon_a2b, real_b)\n",
    "\n",
    "        # Total generator loss\n",
    "        loss_gen_total = loss_gan_gen_a2b + loss_gan_gen_b2a \\\n",
    "            + (loss_cycle_a2b2a + loss_cycle_b2a2b) * self.LAMBDA_CYCLE \\\n",
    "            + (loss_identity_a2b + loss_identity_b2a) * self.LAMBDA_ID\n",
    "\n",
    "        if self.is_training:\n",
    "            # Calculate gradients\n",
    "            loss_gen_total.backward()#retain_graph=True)\n",
    "\n",
    "            # Update G_A and G_B's weights\n",
    "            self.optimizer_G.step()\n",
    "\n",
    "\n",
    "        if self.save_losses:\n",
    "            if self.is_training:\n",
    "                self.tr_gen_loss_dict['loss_gen_a2b'].append(loss_gan_gen_a2b.item())\n",
    "                self.tr_gen_loss_dict['loss_gen_b2a'].append(loss_gan_gen_b2a.item())\n",
    "                self.tr_gen_loss_dict['loss_id_a2b'].append(loss_identity_a2b.item())\n",
    "                self.tr_gen_loss_dict['loss_id_b2a'].append(loss_identity_b2a.item())\n",
    "                self.tr_gen_loss_dict['loss_cycle_a2b2a'].append(loss_cycle_a2b2a.item())\n",
    "                self.tr_gen_loss_dict['loss_cycle_b2a2b'].append(loss_cycle_b2a2b.item())\n",
    "                self.tr_gen_loss_dict['loss_gen_total'].append(loss_gen_total.item())\n",
    "            else:\n",
    "                self.val_gen_loss_dict['loss_gen_a2b'].append(loss_gan_gen_a2b.item())\n",
    "                self.val_gen_loss_dict['loss_gen_b2a'].append(loss_gan_gen_b2a.item())\n",
    "                self.val_gen_loss_dict['loss_id_a2b'].append(loss_identity_a2b.item())\n",
    "                self.val_gen_loss_dict['loss_id_b2a'].append(loss_identity_b2a.item())\n",
    "                self.val_gen_loss_dict['loss_cycle_a2b2a'].append(loss_cycle_a2b2a.item())\n",
    "                self.val_gen_loss_dict['loss_cycle_b2a2b'].append(loss_cycle_b2a2b.item())\n",
    "                self.val_gen_loss_dict['loss_gen_total'].append(loss_gen_total.item())\n",
    "\n",
    "\n",
    "    def backward_D(self, real_a, real_b, fake_a2b, fake_b2a):\n",
    "\n",
    "        # Re-assign fake_a2b and fake_b2a from the image pool.\n",
    "        fake_a2b = self.fake_B_pool.query(fake_a2b)\n",
    "        fake_b2a = self.fake_A_pool.query(fake_b2a)\n",
    "\n",
    "        if self.is_training:\n",
    "            self.set_requires_grad([self.discriminator_a, self.discriminator_b], True)\n",
    "            self.optimizer_D.zero_grad()   # set D_A and D_B's gradients to zero\n",
    "\n",
    "\n",
    "        # Discriminator A should classify real_a as A\n",
    "        loss_gan_dis_a_real = gan_loss(self.discriminator_a(real_a), True)\n",
    "        # Discriminator A should classify generated fake_b2a as not A\n",
    "        loss_gan_dis_a_fake = gan_loss(self.discriminator_a(fake_b2a.detach()), False) # Detach added\n",
    "\n",
    "        # Discriminator B should classify real_b as B\n",
    "        loss_gan_dis_b_real = gan_loss(self.discriminator_b(real_b), True)\n",
    "        # Discriminator B should classify generated fake_a2b as not B\n",
    "        loss_gan_dis_b_fake = gan_loss(self.discriminator_b(fake_a2b.detach()), False) # Detach added\n",
    "\n",
    "        # Total discriminator loss\n",
    "        loss_dis_a = (loss_gan_dis_a_real + loss_gan_dis_a_fake) * 0.5\n",
    "        loss_dis_b = (loss_gan_dis_b_real + loss_gan_dis_b_fake) * 0.5\n",
    "\n",
    "        loss_dis_total = loss_dis_a + loss_dis_b\n",
    "\n",
    "        if self.is_training:\n",
    "            # Calculate gradients\n",
    "            loss_dis_total.backward()\n",
    "            # Update D_A and D_B's weights\n",
    "            self.optimizer_D.step()\n",
    "\n",
    "        # Save train and validation losses separately\n",
    "        if self.save_losses:\n",
    "            if self.is_training:\n",
    "                self.tr_dis_loss_dict['loss_dis_b'].append(loss_dis_b.item())\n",
    "                self.tr_dis_loss_dict['loss_dis_a'].append(loss_dis_a.item())\n",
    "                self.tr_dis_loss_dict['loss_dis_total'].append(loss_dis_total.item())\n",
    "            else:\n",
    "                self.val_dis_loss_dict['loss_dis_b'].append(loss_dis_b.item())\n",
    "                self.val_dis_loss_dict['loss_dis_a'].append(loss_dis_a.item())\n",
    "                self.val_dis_loss_dict['loss_dis_total'].append(loss_dis_total.item())\n",
    "\n",
    "    def set_requires_grad(self, nets, requires_grad=False):\n",
    "        \"\"\"Set requies_grad=False for all the networks to avoid unnecessary computations\n",
    "        Parameters:\n",
    "            nets (network list)   -- a list of networks\n",
    "            requires_grad (bool)  -- whether the networks require gradients or not\n",
    "        \"\"\"\n",
    "        if not isinstance(nets, list):\n",
    "            nets = [nets]\n",
    "        for net in nets:\n",
    "            if net is not None:\n",
    "                for param in net.parameters():\n",
    "                    param.requires_grad = requires_grad\n",
    "\n",
    "    def optimize_parameters(self, real_a, real_b):\n",
    "\n",
    "        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n",
    "        # Forward\n",
    "        fake_a2b, recon_b2a, fake_b2a, recon_a2b, identity_a2b, identity_b2a = self.forward(real_a, real_b)  # compute fake images and reconstruction images.\n",
    "        # G_A and G_B\n",
    "        self.backward_G(real_a, real_b, fake_a2b, recon_b2a, fake_b2a, recon_a2b, identity_a2b, identity_b2a)  # calculate gradients for G_A and G_B\n",
    "        # D_A and D_B\n",
    "        self.backward_D(real_a, real_b, fake_a2b, fake_b2a)  # To-Do: Query fake images from the pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d2a170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_folders():\n",
    "\n",
    "    currentDT = datetime.datetime.now().strftime(\"%Y_%m_%d-%H:%M\")\n",
    "\n",
    "    cur_dir = os.getcwd()\n",
    "\n",
    "    if not os.path.isdir(os.path.join(cur_dir, 'Output')):\n",
    "        os.mkdir(os.path.join(cur_dir, 'Output'))\n",
    "\n",
    "    output_folder = os.path.join(cur_dir, 'Output')\n",
    "    output_folder = os.path.join(output_folder, currentDT)\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "    graph_save_dir = os.path.join(output_folder, 'loss-graphs')\n",
    "    if not os.path.isdir(graph_save_dir):\n",
    "        os.mkdir(graph_save_dir)\n",
    "\n",
    "    im_save_dir = os.path.join(output_folder, 'generated-images')\n",
    "    if not os.path.isdir(im_save_dir):\n",
    "        os.mkdir(im_save_dir)\n",
    "\n",
    "    tr_im_save_dir = os.path.join(im_save_dir, 'train')\n",
    "    if not os.path.isdir(tr_im_save_dir):\n",
    "        os.mkdir(tr_im_save_dir)\n",
    "\n",
    "    val_im_save_dir = os.path.join(im_save_dir, 'val')\n",
    "    if not os.path.isdir(val_im_save_dir):\n",
    "        os.mkdir(val_im_save_dir)\n",
    "\n",
    "    model_save_dir = os.path.join(output_folder, 'saved-models')\n",
    "    if not os.path.isdir(model_save_dir):\n",
    "        os.mkdir(model_save_dir)\n",
    "\n",
    "    # Check if the directories exist\n",
    "    assert(os.path.isdir(im_save_dir)), 'Check your im_save_dir path.'\n",
    "    assert(os.path.isdir(graph_save_dir)), 'Check your graph_save_dir path.'\n",
    "\n",
    "    print('-----Directories to save the output-----\\nTrain Fake Images: {}\\nVal Fake Images: {}\\nLosses: {}\\nModel: {}'.format(tr_im_save_dir, val_im_save_dir, graph_save_dir, model_save_dir))\n",
    "\n",
    "    return tr_im_save_dir, val_im_save_dir, graph_save_dir, model_save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b78d03f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Check your data path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/celeba-dataset/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Make sure that the directory exists\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(data_dir)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCheck your data path.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m tr_pairs, val_pairs \u001b[38;5;241m=\u001b[39m get_image_pairs(data_dir, num_pairs\u001b[38;5;241m=\u001b[39mnum_pairs)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Parameters\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Check your data path."
     ]
    }
   ],
   "source": [
    "data_dir = '/celeba-dataset/'\n",
    "\n",
    "# Make sure that the directory exists\n",
    "assert(os.path.isdir(data_dir)), 'Check your data path.'\n",
    "\n",
    "tr_pairs, val_pairs = get_image_pairs(data_dir, num_pairs=num_pairs)\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 1,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "\n",
    "# Datasets\n",
    "# partition = {'train': pairs['male'], 'validation': data_subset[:10]} # IDs\n",
    "# labels = {'id-1': 0, 'id-2': 1, 'id-3': 2, 'id-4': 1}\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(tr_pairs['males'], tr_pairs['females'])\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(val_pairs['males'], val_pairs['females'])\n",
    "validation_generator = data.DataLoader(validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ae705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, validation_dataset, epochs, device):\n",
    "    \"\"\"\n",
    "    X_m: Real male image from dataset\n",
    "    X_f: Real female image from dataset\n",
    "    y_m: Male label = 1\n",
    "    y_f: Female label = 0\n",
    "    \"\"\"\n",
    "\n",
    "    # Load model into GPU if available\n",
    "    model = cycleGAN().to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print('Epoch', epoch+1, '------------------')\n",
    "\n",
    "        # Training\n",
    "        temp = 1\n",
    "        model.is_training = True\n",
    "        for X_m, X_f, y_m, y_f in train_dataset:\n",
    "\n",
    "            # Send input images to gpu if available.\n",
    "            X_m, X_f = X_m.to(device), X_f.to(device)\n",
    "\n",
    "            # Save loss values at the end of each epoch\n",
    "            if temp == train_dataset.__len__():\n",
    "                model.save_losses = True\n",
    "\n",
    "            model.optimize_parameters(X_m, X_f)\n",
    "\n",
    "            temp = temp+1\n",
    "\n",
    "        print('Tr - Total Generator Loss:', np.round(model.tr_gen_loss_dict['loss_gen_total'][-1], decimals=4))\n",
    "        print('Tr - Total Dicriminator Loss:', np.round(model.tr_dis_loss_dict['loss_dis_total'][-1], decimals=4))\n",
    "\n",
    "        model.save_losses = False\n",
    "\n",
    "        #if epoch % 10 == 0:\n",
    "            # Plot images each 10th epoch.\n",
    "        print_images(model.im_list, tr_im_save_dir, str(epoch), save_mode_on=True)\n",
    "\n",
    "        # Validation\n",
    "        with torch.set_grad_enabled(False):\n",
    "\n",
    "            temp = 1\n",
    "            model.is_training = False\n",
    "            for X_m, X_f, y_m, y_f in validation_dataset:\n",
    "\n",
    "                X_m, X_f = X_m.to(device), X_f.to(device)\n",
    "\n",
    "                if temp == validation_dataset.__len__():\n",
    "                    model.save_losses = True\n",
    "\n",
    "                model.optimize_parameters(X_m, X_f)\n",
    "\n",
    "                temp = temp+1\n",
    "\n",
    "            print('----')\n",
    "            print('Val - Total Generator Loss:', np.round(model.val_gen_loss_dict['loss_gen_total'][-1], decimals=4))\n",
    "            print('Val - Total Dicriminator Loss:', np.round(model.val_dis_loss_dict['loss_dis_total'][-1], decimals=4))\n",
    "\n",
    "            model.save_losses = False\n",
    "\n",
    "            print_images(model.im_list, val_im_save_dir, str(epoch), save_mode_on=True)\n",
    "\n",
    "    # Save gen and disc loss values to respective csv files.\n",
    "    df = pd.DataFrame.from_dict(model.tr_gen_loss_dict)\n",
    "    df.to_csv(os.path.join(graph_save_dir, 'tr_gen_losses.csv'), index=False)\n",
    "    df = pd.DataFrame.from_dict(model.tr_dis_loss_dict)\n",
    "    df.to_csv(os.path.join(graph_save_dir, 'tr_dis_losses.csv'), index=False)\n",
    "    # Save gen and disc loss values to respective csv files.\n",
    "    df = pd.DataFrame.from_dict(model.val_gen_loss_dict)\n",
    "    df.to_csv(os.path.join(graph_save_dir, 'val_gen_losses.csv'), index=False)\n",
    "    df = pd.DataFrame.from_dict(model.val_dis_loss_dict)\n",
    "    df.to_csv(os.path.join(graph_save_dir, 'val_dis_losses.csv'), index=False)\n",
    "\n",
    "    # Save entire model architecture and params.\n",
    "    torch.save(model, os.path.join(model_save_dir, assign_model_id(model_save_dir)) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c284bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_im_save_dir, val_im_save_dir, graph_save_dir, model_save_dir = manage_folders()\n",
    "print('\\n-----Number of male/female image pairs-----\\nTrain:', len(tr_pairs['males']))\n",
    "print('Validation:', len(val_pairs['males']), '\\n')\n",
    "\n",
    "# Start training.\n",
    "train(training_generator, validation_generator, epochs, device)\n",
    "\n",
    "print('Finished.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
